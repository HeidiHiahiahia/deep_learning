{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c6788aef474ca12",
      "metadata": {
        "collapsed": false,
        "id": "7c6788aef474ca12"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks (RNNs)\n",
        "\n",
        "In this assignment, you'll build upon your understanding of RNNs and Keras to develop a word-level text generation model.  Your goal is to train a model that learns the stylistic nuances of a chosen corpus and generates new, original text segments that echo the source material's essence.\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "We've provided several intriguing text corpora to get you started:\n",
        "\n",
        "*   Mark Twain\n",
        "*   Charles Dickens\n",
        "*   William Shakespeare\n",
        "\n",
        "**Feel free to explore!**  If you have a particular passion for another author, genre, or a specific text, you're encouraged to use your own dataset of raw text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "2d0bfedcfe52aedc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0bfedcfe52aedc",
        "outputId": "abb97f3f-cc83-4852-d8f8-35bee363b5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ]
        }
      ],
      "source": [
        "# Check if we have a GPU available\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab51c764031e606",
      "metadata": {
        "collapsed": false,
        "id": "dab51c764031e606"
      },
      "source": [
        "# 1. Data Preparation (Complete or Incomplete)\n",
        "\n",
        "Before we can begin training an RNN model, we need to prepare the dataset. This involves cleaning the text, tokenizing words, and creating sequences the model can be trained on.\n",
        "\n",
        "## 1.1 Data Exploration\n",
        "\n",
        "Print the first 1000 characters of the dataset. Report the dataset's size and the number of unique characters it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3cQ8-z3T7Owa",
      "metadata": {
        "id": "3cQ8-z3T7Owa"
      },
      "outputs": [],
      "source": [
        "def download_file(url, file_path):\n",
        "    import requests\n",
        "    r = requests.get(url)\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "def load_dataset(file_path, fraction=1.0):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "    return raw_text[:int(fraction * len(raw_text))]\n",
        "\n",
        "dataset = 'shakespeare.txt' # Other options are mark_twain.txt, charles_dickens.txt\n",
        "\n",
        "download_file(\"https://raw.githubusercontent.com/HeidiHiahiahia/deep_learning/main/02_activities/assignments/downloaded_books/shakespeare.txt\", dataset)\n",
        "\n",
        "# Load chosen dataset. NOTE: If Colab is running out of memory, change the `fraction` parameter to a value between 0 and 1 to load less data.\n",
        "text = load_dataset(dataset, fraction=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "BunkZmdkl0Wn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BunkZmdkl0Wn",
        "outputId": "376236b4-a8b1-4028-b0e9-d6901077fee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Hamlet, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "Title: Hamlet\n",
            "\n",
            "Author: William Shakespeare\n",
            "\n",
            "Editor: Charles Kean\n",
            "\n",
            "Release Date: January 10, 2009 [EBook #27761]\n",
            "\n",
            "Language: English\n",
            "\n",
            "Character set encoding: UTF-8\n",
            "\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK HAMLET ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Produced by David Starner, Curtis Weyant and the Online\n",
            "Distributed Proofreading Team at https://www.pgdp.net\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Transcriber's Note:\n",
            "    This is a heavily edited version of _Hamlet_. It was used\n",
            "    for Charles Kean's 1859 stage production. Phrases printed\n",
            "    in italics in the book are indicated in this electronic\n",
            "    version by _ (underscore). Footnotes originally appeared\n",
            "    at the bottom of each page. For this electronic version\n",
            "    the footnotes \n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2J8rMfgBTgwk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J8rMfgBTgwk",
        "outputId": "bbc0d06f-c6bd-445d-cb16-6433eb7f9638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28267"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words=text.split()\n",
        "unique_words = set(words)\n",
        "unique_words_count = len(unique_words)\n",
        "unique_words_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae1639f5ecfe587",
      "metadata": {
        "collapsed": false,
        "id": "3ae1639f5ecfe587"
      },
      "source": [
        "sssse## 1.2 Text Pre-Processing\n",
        "\n",
        "To prepare the dataset for training, we need to clean the text and create a numerical representation the model can interpret. Perform the following pre-processing steps:\n",
        "\n",
        "*   Convert the entire text to lowercase.\n",
        "*   Use the `Tokenizer` class from the `keras.preprocessing.text` module to tokenize the text. You should fit the tokenizer on the text and then convert the text to a sequence of numbers. You can use the `texts_to_sequences` method to do this.\n",
        "\n",
        "**Note**:\n",
        "* You'll need to specify an appropriate size for the vocabulary. The number of words in the list of most common words can serve as a guide - does it seem like a reasonable vocabulary size?\n",
        "* Some of the words will be excluded from the vocabulary, as they don't appear often enough. It's important to provide a value for `oov_token` when creating the Tokenizer instance, so that these words can be represented as \"unknown\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "IwoHsy-kEieA",
      "metadata": {
        "id": "IwoHsy-kEieA"
      },
      "outputs": [],
      "source": [
        "text=text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "4d0d30cd98ea453c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d0d30cd98ea453c",
        "outputId": "84d04b60-4152-418b-934f-9fd99ff008de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 13218 unique tokens.\n",
            "Example of word_index: [('<00V>', 1), ('the', 2), ('and', 3), ('of', 4), ('to', 5)]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Your code here\n",
        "\n",
        "VOCAB_SIZE = 1400\n",
        "OOV_TOKEN = \"<00V>\"\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE,oov_token=OOV_TOKEN,char_level=False)\n",
        "#,\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "#sequences_test = tokenizer.texts_to_sequences(\n",
        "#sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print(f'Example of word_index: {list(word_index.items())[:5]}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "Cc-U_PtwxTrF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc-U_PtwxTrF",
        "outputId": "0dc25aa9-3449-4243-8c78-415f8ef2c2cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2,\n",
              "  56,\n",
              "  54,\n",
              "  411,\n",
              "  4,\n",
              "  107,\n",
              "  28,\n",
              "  668,\n",
              "  172,\n",
              "  20,\n",
              "  411,\n",
              "  13,\n",
              "  16,\n",
              "  2,\n",
              "  176,\n",
              "  4,\n",
              "  669,\n",
              "  1,\n",
              "  33,\n",
              "  43,\n",
              "  1095,\n",
              "  3,\n",
              "  14,\n",
              "  412,\n",
              "  43,\n",
              "  1,\n",
              "  1,\n",
              "  10,\n",
              "  85,\n",
              "  333,\n",
              "  15,\n",
              "  124,\n",
              "  15,\n",
              "  163,\n",
              "  29,\n",
              "  531,\n",
              "  176,\n",
              "  15,\n",
              "  254,\n",
              "  2,\n",
              "  216,\n",
              "  4,\n",
              "  2,\n",
              "  56,\n",
              "  54,\n",
              "  297,\n",
              "  1248,\n",
              "  14,\n",
              "  20,\n",
              "  411,\n",
              "  29,\n",
              "  759,\n",
              "  33,\n",
              "  592,\n",
              "  54,\n",
              "  366,\n",
              "  785,\n",
              "  107,\n",
              "  1,\n",
              "  668,\n",
              "  172,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  564,\n",
              "  1,\n",
              "  532,\n",
              "  1,\n",
              "  411,\n",
              "  1,\n",
              "  989,\n",
              "  609,\n",
              "  533,\n",
              "  177,\n",
              "  1,\n",
              "  1,\n",
              "  462,\n",
              "  874,\n",
              "  4,\n",
              "  20,\n",
              "  56,\n",
              "  54,\n",
              "  411,\n",
              "  107,\n",
              "  875,\n",
              "  28,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  2,\n",
              "  759,\n",
              "  733,\n",
              "  1,\n",
              "  1,\n",
              "  33,\n",
              "  1,\n",
              "  592,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  207,\n",
              "  20,\n",
              "  13,\n",
              "  6,\n",
              "  1,\n",
              "  1,\n",
              "  1096,\n",
              "  4,\n",
              "  107,\n",
              "  15,\n",
              "  18,\n",
              "  363,\n",
              "  16,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  565,\n",
              "  1,\n",
              "  1,\n",
              "  396,\n",
              "  7,\n",
              "  1,\n",
              "  7,\n",
              "  2,\n",
              "  990,\n",
              "  40,\n",
              "  1,\n",
              "  7,\n",
              "  20,\n",
              "  183,\n",
              "  1096,\n",
              "  28,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  734,\n",
              "  33,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  310,\n",
              "  499,\n",
              "  16,\n",
              "  20,\n",
              "  183,\n",
              "  1096,\n",
              "  2,\n",
              "  1,\n",
              "  40,\n",
              "  1,\n",
              "  33,\n",
              "  2,\n",
              "  208,\n",
              "  4,\n",
              "  310,\n",
              "  204,\n",
              "  7,\n",
              "  204,\n",
              "  8,\n",
              "  142,\n",
              "  292,\n",
              "  2,\n",
              "  276,\n",
              "  1,\n",
              "  157,\n",
              "  101,\n",
              "  1097,\n",
              "  5,\n",
              "  1,\n",
              "  6,\n",
              "  1,\n",
              "  1,\n",
              "  18,\n",
              "  1181,\n",
              "  5,\n",
              "  204,\n",
              "  149,\n",
              "  42,\n",
              "  876,\n",
              "  168,\n",
              "  1,\n",
              "  35,\n",
              "  31,\n",
              "  991,\n",
              "  6,\n",
              "  1,\n",
              "  1,\n",
              "  18,\n",
              "  1181,\n",
              "  5,\n",
              "  204,\n",
              "  149,\n",
              "  42,\n",
              "  437,\n",
              "  168,\n",
              "  1,\n",
              "  1,\n",
              "  2,\n",
              "  276,\n",
              "  709,\n",
              "  1,\n",
              "  840,\n",
              "  7,\n",
              "  204,\n",
              "  139,\n",
              "  142,\n",
              "  169,\n",
              "  7,\n",
              "  42,\n",
              "  139,\n",
              "  1325,\n",
              "  1,\n",
              "  840,\n",
              "  155,\n",
              "  6,\n",
              "  1,\n",
              "  199,\n",
              "  40,\n",
              "  22,\n",
              "  47,\n",
              "  927,\n",
              "  7,\n",
              "  2,\n",
              "  990,\n",
              "  306,\n",
              "  1326,\n",
              "  4,\n",
              "  107,\n",
              "  534,\n",
              "  4,\n",
              "  670,\n",
              "  1,\n",
              "  16,\n",
              "  1,\n",
              "  33,\n",
              "  2,\n",
              "  1098,\n",
              "  1,\n",
              "  1,\n",
              "  14,\n",
              "  1,\n",
              "  382,\n",
              "  28,\n",
              "  1,\n",
              "  1,\n",
              "  397,\n",
              "  72,\n",
              "  6,\n",
              "  22,\n",
              "  1,\n",
              "  35,\n",
              "  1,\n",
              "  1,\n",
              "  532,\n",
              "  1,\n",
              "  1249,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  786,\n",
              "  1,\n",
              "  992,\n",
              "  1,\n",
              "  1249,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1327,\n",
              "  52,\n",
              "  4,\n",
              "  670,\n",
              "  645,\n",
              "  1,\n",
              "  107,\n",
              "  238,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  5,\n",
              "  2,\n",
              "  610,\n",
              "  52,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  406,\n",
              "  89,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  319,\n",
              "  240,\n",
              "  5,\n",
              "  107,\n",
              "  645,\n",
              "  1,\n",
              "  367,\n",
              "  238,\n",
              "  5,\n",
              "  406,\n",
              "  645,\n",
              "  1,\n",
              "  397,\n",
              "  1,\n",
              "  760,\n",
              "  645,\n",
              "  1,\n",
              "  710,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1099,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  290,\n",
              "  4,\n",
              "  818,\n",
              "  146,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  143,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  377,\n",
              "  1,\n",
              "  645,\n",
              "  161,\n",
              "  1,\n",
              "  143,\n",
              "  1,\n",
              "  645,\n",
              "  397,\n",
              "  1,\n",
              "  377,\n",
              "  1,\n",
              "  645,\n",
              "  1,\n",
              "  1,\n",
              "  114,\n",
              "  4,\n",
              "  670,\n",
              "  3,\n",
              "  355,\n",
              "  4,\n",
              "  107,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  413,\n",
              "  194,\n",
              "  4,\n",
              "  406,\n",
              "  1,\n",
              "  1100,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  565,\n",
              "  1,\n",
              "  131,\n",
              "  161,\n",
              "  281,\n",
              "  265,\n",
              "  248,\n",
              "  141,\n",
              "  161,\n",
              "  241,\n",
              "  248,\n",
              "  691,\n",
              "  50,\n",
              "  1,\n",
              "  1,\n",
              "  131,\n",
              "  161,\n",
              "  148,\n",
              "  1,\n",
              "  398,\n",
              "  2,\n",
              "  1,\n",
              "  39,\n",
              "  2,\n",
              "  265,\n",
              "  248,\n",
              "  141,\n",
              "  161,\n",
              "  148,\n",
              "  1,\n",
              "  398,\n",
              "  2,\n",
              "  1,\n",
              "  39,\n",
              "  2,\n",
              "  241,\n",
              "  248,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  51,\n",
              "  35,\n",
              "  2,\n",
              "  565,\n",
              "  131,\n",
              "  281,\n",
              "  35,\n",
              "  2,\n",
              "  265,\n",
              "  877,\n",
              "  4,\n",
              "  2,\n",
              "  565,\n",
              "  141,\n",
              "  35,\n",
              "  2,\n",
              "  241,\n",
              "  877,\n",
              "  4,\n",
              "  2,\n",
              "  565,\n",
              "  148,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  565,\n",
              "  131,\n",
              "  148,\n",
              "  265,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  565,\n",
              "  141,\n",
              "  148,\n",
              "  241,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  565,\n",
              "  2,\n",
              "  1,\n",
              "  13,\n",
              "  993,\n",
              "  5,\n",
              "  27,\n",
              "  35,\n",
              "  2,\n",
              "  565,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  1328,\n",
              "  2,\n",
              "  158,\n",
              "  4,\n",
              "  107,\n",
              "  13,\n",
              "  711,\n",
              "  37,\n",
              "  511,\n",
              "  2,\n",
              "  112,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  306,\n",
              "  1329,\n",
              "  1,\n",
              "  22,\n",
              "  6,\n",
              "  1,\n",
              "  5,\n",
              "  692,\n",
              "  2,\n",
              "  693,\n",
              "  3,\n",
              "  1250,\n",
              "  4,\n",
              "  2,\n",
              "  315,\n",
              "  3,\n",
              "  22,\n",
              "  6,\n",
              "  1,\n",
              "  5,\n",
              "  1101,\n",
              "  1,\n",
              "  9,\n",
              "  2,\n",
              "  307,\n",
              "  4,\n",
              "  211,\n",
              "  1,\n",
              "  18,\n",
              "  1330,\n",
              "  28,\n",
              "  237,\n",
              "  566,\n",
              "  96,\n",
              "  1,\n",
              "  994,\n",
              "  14,\n",
              "  211,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  878,\n",
              "  1,\n",
              "  277,\n",
              "  14,\n",
              "  211,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  6,\n",
              "  383,\n",
              "  184,\n",
              "  3,\n",
              "  1,\n",
              "  291,\n",
              "  14,\n",
              "  211,\n",
              "  1,\n",
              "  611,\n",
              "  4,\n",
              "  6,\n",
              "  1,\n",
              "  262,\n",
              "  1,\n",
              "  1,\n",
              "  83,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  223,\n",
              "  2,\n",
              "  928,\n",
              "  1,\n",
              "  4,\n",
              "  12,\n",
              "  1,\n",
              "  209,\n",
              "  1,\n",
              "  1,\n",
              "  7,\n",
              "  2,\n",
              "  112,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  37,\n",
              "  244,\n",
              "  1,\n",
              "  1,\n",
              "  49,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  1,\n",
              "  242,\n",
              "  2,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  3,\n",
              "  2,\n",
              "  1,\n",
              "  107,\n",
              "  13,\n",
              "  6,\n",
              "  879,\n",
              "  4,\n",
              "  307,\n",
              "  6,\n",
              "  1326,\n",
              "  4,\n",
              "  178,\n",
              "  15,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  112,\n",
              "  1,\n",
              "  1331,\n",
              "  150,\n",
              "  1102,\n",
              "  2,\n",
              "  989,\n",
              "  4,\n",
              "  2,\n",
              "  189,\n",
              "  1,\n",
              "  2,\n",
              "  841,\n",
              "  1,\n",
              "  4,\n",
              "  320,\n",
              "  512,\n",
              "  3,\n",
              "  1332,\n",
              "  90,\n",
              "  55,\n",
              "  30,\n",
              "  43,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  533,\n",
              "  23,\n",
              "  152,\n",
              "  14,\n",
              "  211,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  6,\n",
              "  581,\n",
              "  383,\n",
              "  1,\n",
              "  28,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  173,\n",
              "  1,\n",
              "  92,\n",
              "  4,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  2,\n",
              "  929,\n",
              "  384,\n",
              "  4,\n",
              "  107,\n",
              "  13,\n",
              "  5,\n",
              "  27,\n",
              "  220,\n",
              "  7,\n",
              "  2,\n",
              "  1251,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  57,\n",
              "  463,\n",
              "  7,\n",
              "  2,\n",
              "  694,\n",
              "  1,\n",
              "  1252,\n",
              "  2,\n",
              "  208,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  1182,\n",
              "  2,\n",
              "  735,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  116,\n",
              "  6,\n",
              "  995,\n",
              "  4,\n",
              "  1,\n",
              "  49,\n",
              "  59,\n",
              "  1,\n",
              "  116,\n",
              "  609,\n",
              "  3,\n",
              "  396,\n",
              "  7,\n",
              "  6,\n",
              "  880,\n",
              "  1,\n",
              "  996,\n",
              "  451,\n",
              "  881,\n",
              "  254,\n",
              "  2,\n",
              "  785,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  1,\n",
              "  39,\n",
              "  49,\n",
              "  1,\n",
              "  172,\n",
              "  1,\n",
              "  2,\n",
              "  610,\n",
              "  1326,\n",
              "  1,\n",
              "  157,\n",
              "  1,\n",
              "  12,\n",
              "  879,\n",
              "  164,\n",
              "  1,\n",
              "  364,\n",
              "  144,\n",
              "  1,\n",
              "  51,\n",
              "  1,\n",
              "  1,\n",
              "  7,\n",
              "  1,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  628,\n",
              "  157,\n",
              "  34,\n",
              "  464,\n",
              "  1,\n",
              "  2,\n",
              "  761,\n",
              "  22,\n",
              "  5,\n",
              "  102,\n",
              "  107,\n",
              "  6,\n",
              "  1,\n",
              "  3,\n",
              "  593,\n",
              "  1,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  787,\n",
              "  4,\n",
              "  670,\n",
              "  6,\n",
              "  564,\n",
              "  133,\n",
              "  263,\n",
              "  27,\n",
              "  1,\n",
              "  1,\n",
              "  3,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  3,\n",
              "  1,\n",
              "  1,\n",
              "  85,\n",
              "  27,\n",
              "  1,\n",
              "  16,\n",
              "  2,\n",
              "  476,\n",
              "  64,\n",
              "  40,\n",
              "  23,\n",
              "  736,\n",
              "  1,\n",
              "  1,\n",
              "  7,\n",
              "  1,\n",
              "  23,\n",
              "  97,\n",
              "  736,\n",
              "  1,\n",
              "  513,\n",
              "  5,\n",
              "  489,\n",
              "  9,\n",
              "  103,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  368,\n",
              "  2,\n",
              "  1333,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  3,\n",
              "  9,\n",
              "  4,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  2,\n",
              "  266,\n",
              "  1,\n",
              "  293,\n",
              "  211,\n",
              "  143,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  136,\n",
              "  1,\n",
              "  3,\n",
              "  6,\n",
              "  788,\n",
              "  1,\n",
              "  43,\n",
              "  158,\n",
              "  157,\n",
              "  101,\n",
              "  1,\n",
              "  34,\n",
              "  1,\n",
              "  29,\n",
              "  1,\n",
              "  127,\n",
              "  1,\n",
              "  1250,\n",
              "  15,\n",
              "  1,\n",
              "  221,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  211,\n",
              "  1,\n",
              "  199,\n",
              "  2,\n",
              "  1,\n",
              "  3,\n",
              "  2,\n",
              "  1,\n",
              "  15,\n",
              "  1,\n",
              "  6,\n",
              "  1,\n",
              "  7,\n",
              "  320,\n",
              "  1,\n",
              "  628,\n",
              "  762,\n",
              "  29,\n",
              "  628,\n",
              "  1,\n",
              "  28,\n",
              "  211,\n",
              "  1,\n",
              "  1,\n",
              "  15,\n",
              "  1,\n",
              "  2,\n",
              "  1,\n",
              "  4,\n",
              "  593,\n",
              "  711,\n",
              "  9,\n",
              "  4,\n",
              "  320,\n",
              "  1,\n",
              "  930,\n",
              "  29,\n",
              "  610,\n",
              "  15,\n",
              "  13,\n",
              "  233,\n",
              "  2,\n",
              "  112,\n",
              "  1,\n",
              "  1,\n",
              "  4,\n",
              "  878,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  107,\n",
              "  534,\n",
              "  4,\n",
              "  670,\n",
              "  204,\n",
              "  8,\n",
              "  142,\n",
              "  8,\n",
              "  1,\n",
              "  6,\n",
              "  1,\n",
              "  144,\n",
              "  2,\n",
              "  1183,\n",
              "  180,\n",
              "  1,\n",
              "  35,\n",
              "  12,\n",
              "  1,\n",
              "  128,\n",
              "  5,\n",
              "  25,\n",
              "  1,\n",
              "  141,\n",
              "  161,\n",
              "  931,\n",
              "  1,\n",
              "  64,\n",
              "  1,\n",
              "  131,\n",
              "  389,\n",
              "  414,\n",
              "  26,\n",
              "  63,\n",
              "  594,\n",
              "  3,\n",
              "  1,\n",
              "  156,\n",
              "  582,\n",
              "  931,\n",
              "  228,\n",
              "  342,\n",
              "  2,\n",
              "  52,\n",
              "  169,\n",
              "  1,\n",
              "  1,\n",
              "  931,\n",
              "  11,\n",
              "  1,\n",
              "  10,\n",
              "  80,\n",
              "  112,\n",
              "  1,\n",
              "  99,\n",
              "  31,\n",
              "  452,\n",
              "  931,\n",
              "  298,\n",
              "  61,\n",
              "  1184,\n",
              "  1334,\n",
              "  343,\n",
              "  78,\n",
              "  5,\n",
              "  477,\n",
              "  1,\n",
              "  1,\n",
              "  16,\n",
              "  20,\n",
              "  1,\n",
              "  137,\n",
              "  1,\n",
              "  550,\n",
              "  5,\n",
              "  141,\n",
              "  298,\n",
              "  997,\n",
              "  789,\n",
              "  3,\n",
              "  8,\n",
              "  ...]]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d32bb9356f711",
      "metadata": {
        "collapsed": false,
        "id": "89d32bb9356f711"
      },
      "source": [
        "If everything worked, the following line should show you the first 10 words in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "6a7cd547a19feece",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a7cd547a19feece",
        "outputId": "b135d0ed-c60e-4a68-fac8-1b696015c88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<00V>', 1), ('the', 2), ('and', 3), ('of', 4), ('to', 5), ('a', 6), ('in', 7), ('i', 8), ('that', 9), ('you', 10)]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(list(tokenizer.word_index.items())[:10])\n",
        "except AttributeError:\n",
        "    print(\"Tokenizer has not been initialized. Possible issue: Complete the relevant section of the assignment to initialize it.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da504e4bc6617613",
      "metadata": {
        "collapsed": false,
        "id": "da504e4bc6617613"
      },
      "source": [
        "## 1.3 Sequence Generation\n",
        "\n",
        "Now that the text has been tokenized, we need to create sequences the model can be trained on. There are two parts to this:\n",
        "\n",
        "*   Use the `texts_to_sequences` method from the tokenizer to convert the text to a list of sequences of numbers.\n",
        "*   Generate the training sequences. Each training sequence should contain `SEQ_LENGTH` token IDs from the text. The target token for each sequence should be the word that follows the sequence in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "4lNi-rg8Je6X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lNi-rg8Je6X",
        "outputId": "087cbe13-8f20-441f-a494-5647a5dc8722"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<00V>': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'a': 6,\n",
              " 'in': 7,\n",
              " 'i': 8,\n",
              " 'that': 9,\n",
              " 'you': 10,\n",
              " 'he': 11,\n",
              " 'his': 12,\n",
              " 'is': 13,\n",
              " 'with': 14,\n",
              " 'it': 15,\n",
              " 'for': 16,\n",
              " 'my': 17,\n",
              " 'was': 18,\n",
              " 'not': 19,\n",
              " 'this': 20,\n",
              " '”': 21,\n",
              " 'as': 22,\n",
              " 'but': 23,\n",
              " 'her': 24,\n",
              " 'him': 25,\n",
              " 'me': 26,\n",
              " 'be': 27,\n",
              " 'by': 28,\n",
              " 'or': 29,\n",
              " 'have': 30,\n",
              " 'your': 31,\n",
              " 'she': 32,\n",
              " 'at': 33,\n",
              " 'so': 34,\n",
              " 'on': 35,\n",
              " 'had': 36,\n",
              " 'all': 37,\n",
              " 'said': 38,\n",
              " 'from': 39,\n",
              " 'are': 40,\n",
              " 'will': 41,\n",
              " 'footnote': 42,\n",
              " 'no': 43,\n",
              " 'thou': 44,\n",
              " 'if': 45,\n",
              " 'what': 46,\n",
              " 'they': 47,\n",
              " 'do': 48,\n",
              " 'which': 49,\n",
              " 'e': 50,\n",
              " 'when': 51,\n",
              " 'king': 52,\n",
              " 'would': 53,\n",
              " 'gutenberg': 54,\n",
              " 'we': 55,\n",
              " 'project': 56,\n",
              " 'who': 57,\n",
              " 'then': 58,\n",
              " 'were': 59,\n",
              " 'them': 60,\n",
              " 'now': 61,\n",
              " 'an': 62,\n",
              " '1': 63,\n",
              " 'there': 64,\n",
              " 'their': 65,\n",
              " 'one': 66,\n",
              " 'more': 67,\n",
              " 'thy': 68,\n",
              " 'sir': 69,\n",
              " 'ham': 70,\n",
              " 'man': 71,\n",
              " 's': 72,\n",
              " 'shall': 73,\n",
              " 'any': 74,\n",
              " 'iii': 75,\n",
              " 'how': 76,\n",
              " 'ant': 77,\n",
              " 'thee': 78,\n",
              " 'love': 79,\n",
              " 'come': 80,\n",
              " 'did': 81,\n",
              " 'f1': 82,\n",
              " 'like': 83,\n",
              " 'see': 84,\n",
              " 'may': 85,\n",
              " 'good': 86,\n",
              " 'pope': 87,\n",
              " 'f2': 88,\n",
              " 'lord': 89,\n",
              " 'here': 90,\n",
              " 'f4': 91,\n",
              " 'out': 92,\n",
              " 'f3': 93,\n",
              " 'work': 94,\n",
              " 'tm': 95,\n",
              " 'than': 96,\n",
              " 'these': 97,\n",
              " 'our': 98,\n",
              " 'upon': 99,\n",
              " 'time': 100,\n",
              " 'been': 101,\n",
              " 'make': 102,\n",
              " 'very': 103,\n",
              " 'well': 104,\n",
              " 'ii': 105,\n",
              " 'go': 106,\n",
              " 'hamlet': 107,\n",
              " 'am': 108,\n",
              " 'o': 109,\n",
              " 'must': 110,\n",
              " 'conj': 111,\n",
              " 'most': 112,\n",
              " 'some': 113,\n",
              " 'queen': 114,\n",
              " 'us': 115,\n",
              " 'into': 116,\n",
              " 'where': 117,\n",
              " 'let': 118,\n",
              " 'could': 119,\n",
              " 'too': 120,\n",
              " 'should': 121,\n",
              " 'dro': 122,\n",
              " 'duke': 123,\n",
              " 'give': 124,\n",
              " 'know': 125,\n",
              " 'say': 126,\n",
              " 'such': 127,\n",
              " 'enter': 128,\n",
              " 'ff': 129,\n",
              " 'made': 130,\n",
              " 'r': 131,\n",
              " 'day': 132,\n",
              " 'can': 133,\n",
              " '“i': 134,\n",
              " 'other': 135,\n",
              " 'two': 136,\n",
              " 'much': 137,\n",
              " 'own': 138,\n",
              " 'v': 139,\n",
              " 'up': 140,\n",
              " 'l': 141,\n",
              " 'scene': 142,\n",
              " 'first': 143,\n",
              " 'before': 144,\n",
              " 'works': 145,\n",
              " 'father': 146,\n",
              " 'wife': 147,\n",
              " 'c': 148,\n",
              " 'iv': 149,\n",
              " 'yet': 150,\n",
              " 'came': 151,\n",
              " 'life': 152,\n",
              " 'capell': 153,\n",
              " 'hath': 154,\n",
              " 'without': 155,\n",
              " '2': 156,\n",
              " 'has': 157,\n",
              " 'play': 158,\n",
              " 'take': 159,\n",
              " 'cæsar': 160,\n",
              " 'h': 161,\n",
              " 'again': 162,\n",
              " 'away': 163,\n",
              " 'about': 164,\n",
              " 'told': 165,\n",
              " 'pros': 166,\n",
              " 'old': 167,\n",
              " 'after': 168,\n",
              " '3': 169,\n",
              " 'never': 170,\n",
              " 'only': 171,\n",
              " 'shakespeare': 172,\n",
              " 'thus': 173,\n",
              " 'speak': 174,\n",
              " 'being': 175,\n",
              " 'use': 176,\n",
              " 'set': 177,\n",
              " 'thought': 178,\n",
              " 'mine': 179,\n",
              " 'night': 180,\n",
              " 'nothing': 181,\n",
              " 'why': 182,\n",
              " 'electronic': 183,\n",
              " 'great': 184,\n",
              " 'whom': 185,\n",
              " 'saw': 186,\n",
              " 'tell': 187,\n",
              " 'master': 188,\n",
              " 'heart': 189,\n",
              " 'himself': 190,\n",
              " 'rowe': 191,\n",
              " 'think': 192,\n",
              " 'went': 193,\n",
              " 'daughter': 194,\n",
              " 'part': 195,\n",
              " 'husband': 196,\n",
              " 'men': 197,\n",
              " 'nor': 198,\n",
              " 'both': 199,\n",
              " 'dead': 200,\n",
              " 'death': 201,\n",
              " 'brutus': 202,\n",
              " 'though': 203,\n",
              " 'act': 204,\n",
              " 'i’ll': 205,\n",
              " 'full': 206,\n",
              " 'note': 207,\n",
              " 'end': 208,\n",
              " 'off': 209,\n",
              " 'while': 210,\n",
              " 'its': 211,\n",
              " 'might': 212,\n",
              " 'brother': 213,\n",
              " 'look': 214,\n",
              " 'hanmer': 215,\n",
              " 'terms': 216,\n",
              " 'cannot': 217,\n",
              " 'put': 218,\n",
              " 'adr': 219,\n",
              " 'found': 220,\n",
              " 'within': 221,\n",
              " 'foundation': 222,\n",
              " 'against': 223,\n",
              " 'hear': 224,\n",
              " 'once': 225,\n",
              " 'true': 226,\n",
              " 'still': 227,\n",
              " 'long': 228,\n",
              " 'sea': 229,\n",
              " 'even': 230,\n",
              " 'dromio': 231,\n",
              " 'till': 232,\n",
              " 'indeed': 233,\n",
              " 'name': 234,\n",
              " 'eyes': 235,\n",
              " 'many': 236,\n",
              " 'little': 237,\n",
              " 'son': 238,\n",
              " 'poor': 239,\n",
              " 'friend': 240,\n",
              " 'left': 241,\n",
              " 'art': 242,\n",
              " 'home': 243,\n",
              " 'those': 244,\n",
              " 'sweet': 245,\n",
              " 'theobald': 246,\n",
              " '’': 247,\n",
              " 'hand': 248,\n",
              " 'hor': 249,\n",
              " 'seb': 250,\n",
              " 'head': 251,\n",
              " 'way': 252,\n",
              " 'antipholus': 253,\n",
              " 'under': 254,\n",
              " 'exit': 255,\n",
              " 'last': 256,\n",
              " 'done': 257,\n",
              " 'words': 258,\n",
              " '4': 259,\n",
              " 'gave': 260,\n",
              " 'asked': 261,\n",
              " 'whose': 262,\n",
              " 'therefore': 263,\n",
              " 'money': 264,\n",
              " 'right': 265,\n",
              " 'same': 266,\n",
              " 'heaven': 267,\n",
              " 'dear': 268,\n",
              " 'forth': 269,\n",
              " 'given': 270,\n",
              " 'agreement': 271,\n",
              " 'om': 272,\n",
              " 'sent': 273,\n",
              " 'mad': 274,\n",
              " 'lady': 275,\n",
              " 'word': 276,\n",
              " 'othello': 277,\n",
              " 'place': 278,\n",
              " 'ever': 279,\n",
              " 'house': 280,\n",
              " 'means': 281,\n",
              " 'thing': 282,\n",
              " 'fair': 283,\n",
              " 'bear': 284,\n",
              " 'pol': 285,\n",
              " 'down': 286,\n",
              " 'marry': 287,\n",
              " 'another': 288,\n",
              " 'collier': 289,\n",
              " 'ghost': 290,\n",
              " 'macbeth': 291,\n",
              " '5': 292,\n",
              " 'since': 293,\n",
              " 'leave': 294,\n",
              " 'called': 295,\n",
              " 'timon': 296,\n",
              " 'license': 297,\n",
              " \"'tis\": 298,\n",
              " 'ariel': 299,\n",
              " '244': 300,\n",
              " 'claudio': 301,\n",
              " 'exeunt': 302,\n",
              " 'does': 303,\n",
              " 'help': 304,\n",
              " 'ste': 305,\n",
              " \"shakespeare's\": 306,\n",
              " 'mind': 307,\n",
              " 'ay': 308,\n",
              " 'back': 309,\n",
              " 'each': 310,\n",
              " 'julius': 311,\n",
              " 'lost': 312,\n",
              " 'people': 313,\n",
              " 'ephesus': 314,\n",
              " 'world': 315,\n",
              " 'doth': 316,\n",
              " 'loved': 317,\n",
              " '“you': 318,\n",
              " 'horatio': 319,\n",
              " 'every': 320,\n",
              " 'chain': 321,\n",
              " 'state': 322,\n",
              " \"i'll\": 323,\n",
              " 'spirit': 324,\n",
              " 'better': 325,\n",
              " 'because': 326,\n",
              " 'donations': 327,\n",
              " 'pray': 328,\n",
              " 'keep': 329,\n",
              " 'new': 330,\n",
              " 'find': 331,\n",
              " 'states': 332,\n",
              " 'copy': 333,\n",
              " 'charge': 334,\n",
              " 'face': 335,\n",
              " 'soul': 336,\n",
              " 'prospero': 337,\n",
              " 'heard': 338,\n",
              " 'call': 339,\n",
              " 'copyright': 340,\n",
              " 'ms': 341,\n",
              " 'live': 342,\n",
              " 'get': 343,\n",
              " 'fear': 344,\n",
              " 'strange': 345,\n",
              " 'things': 346,\n",
              " 'woman': 347,\n",
              " 'follow': 348,\n",
              " 'free': 349,\n",
              " 'speech': 350,\n",
              " 'cried': 351,\n",
              " 'literary': 352,\n",
              " 'archive': 353,\n",
              " 'mir': 354,\n",
              " 'mother': 355,\n",
              " 'kind': 356,\n",
              " 'none': 357,\n",
              " 'three': 358,\n",
              " 'ari': 359,\n",
              " 'cal': 360,\n",
              " 'angelo': 361,\n",
              " \"'\": 362,\n",
              " 'used': 363,\n",
              " 'years': 364,\n",
              " 'gon': 365,\n",
              " 'org': 366,\n",
              " 'laertes': 367,\n",
              " 'between': 368,\n",
              " 'seen': 369,\n",
              " 'gone': 370,\n",
              " 'young': 371,\n",
              " 'best': 372,\n",
              " 'alone': 373,\n",
              " 'steevens': 374,\n",
              " '229': 375,\n",
              " 'syracuse': 376,\n",
              " 'second': 377,\n",
              " 'else': 378,\n",
              " 'mistress': 379,\n",
              " 'line': 380,\n",
              " 'ring': 381,\n",
              " 'notes': 382,\n",
              " 'nature': 383,\n",
              " 'story': 384,\n",
              " 'friends': 385,\n",
              " 'die': 386,\n",
              " 'bring': 387,\n",
              " 'benedick': 388,\n",
              " 'nay': 389,\n",
              " 'earth': 390,\n",
              " 'sister': 391,\n",
              " 'took': 392,\n",
              " 'grace': 393,\n",
              " 'valentine': 394,\n",
              " 'monster': 395,\n",
              " 'printed': 396,\n",
              " 'f': 397,\n",
              " 'through': 398,\n",
              " 'together': 399,\n",
              " 'hast': 400,\n",
              " 'makes': 401,\n",
              " 'sword': 402,\n",
              " 'over': 403,\n",
              " 'knew': 404,\n",
              " 'public': 405,\n",
              " 'polonius': 406,\n",
              " 'laer': 407,\n",
              " 'noble': 408,\n",
              " 'oph': 409,\n",
              " '1st': 410,\n",
              " 'ebook': 411,\n",
              " 'almost': 412,\n",
              " 'ophelia': 413,\n",
              " 'answer': 414,\n",
              " 'hold': 415,\n",
              " '15': 416,\n",
              " 'grief': 417,\n",
              " 'cause': 418,\n",
              " 'plays': 419,\n",
              " 'comes': 420,\n",
              " 'form': 421,\n",
              " 'servant': 422,\n",
              " 'enough': 423,\n",
              " 'gold': 424,\n",
              " 'johnson': 425,\n",
              " 'luc': 426,\n",
              " 'paragraph': 427,\n",
              " '234': 428,\n",
              " 'trin': 429,\n",
              " 'romeo': 430,\n",
              " 'cassio': 431,\n",
              " 'cassius': 432,\n",
              " 'course': 433,\n",
              " 'action': 434,\n",
              " 'soon': 435,\n",
              " 'trademark': 436,\n",
              " '50': 437,\n",
              " 'myself': 438,\n",
              " 'return': 439,\n",
              " 'married': 440,\n",
              " 'met': 441,\n",
              " '60': 442,\n",
              " 'wicked': 443,\n",
              " 'rest': 444,\n",
              " 'hope': 445,\n",
              " 'sleep': 446,\n",
              " 'brought': 447,\n",
              " 'clo': 448,\n",
              " 'received': 449,\n",
              " 'iago': 450,\n",
              " 'letter': 451,\n",
              " 'hour': 452,\n",
              " 'just': 453,\n",
              " 'having': 454,\n",
              " 'ship': 455,\n",
              " 'don': 456,\n",
              " 'editions': 457,\n",
              " 'agree': 458,\n",
              " 'alon': 459,\n",
              " 'merchant': 460,\n",
              " 'proteus': 461,\n",
              " '8': 462,\n",
              " 'died': 463,\n",
              " 'far': 464,\n",
              " 'something': 465,\n",
              " 'show': 466,\n",
              " '20': 467,\n",
              " '30': 468,\n",
              " 'days': 469,\n",
              " 'matter': 470,\n",
              " 'access': 471,\n",
              " 'rather': 472,\n",
              " 'edition': 473,\n",
              " 'text': 474,\n",
              " 'helena': 475,\n",
              " 'purpose': 476,\n",
              " 'bed': 477,\n",
              " 'air': 478,\n",
              " 'whole': 479,\n",
              " 'body': 480,\n",
              " 'blood': 481,\n",
              " 'hands': 482,\n",
              " 'sure': 483,\n",
              " 'ill': 484,\n",
              " 'paid': 485,\n",
              " 'united': 486,\n",
              " '’tis': 487,\n",
              " 'hero': 488,\n",
              " 'believe': 489,\n",
              " 'room': 490,\n",
              " '35': 491,\n",
              " 'please': 492,\n",
              " 'light': 493,\n",
              " 'dost': 494,\n",
              " 'refund': 495,\n",
              " 'laws': 496,\n",
              " 'milan': 497,\n",
              " 'imogen': 498,\n",
              " 'page': 499,\n",
              " 'read': 500,\n",
              " 'mean': 501,\n",
              " 'hair': 502,\n",
              " 'business': 503,\n",
              " 'herself': 504,\n",
              " 'thousand': 505,\n",
              " 'lie': 506,\n",
              " 'hence': 507,\n",
              " 'pretty': 508,\n",
              " 'desdemona': 509,\n",
              " 'antony': 510,\n",
              " 'others': 511,\n",
              " 'sense': 512,\n",
              " 'reason': 513,\n",
              " 'welcome': 514,\n",
              " 'stay': 515,\n",
              " 'thine': 516,\n",
              " 'aside': 517,\n",
              " 'eye': 518,\n",
              " 'bound': 519,\n",
              " '45': 520,\n",
              " 'drink': 521,\n",
              " 'truth': 522,\n",
              " 'fool': 523,\n",
              " 'grave': 524,\n",
              " 'verse': 525,\n",
              " 'anon': 526,\n",
              " 'gentleman': 527,\n",
              " 'sebastian': 528,\n",
              " 'antonio': 529,\n",
              " 'fer': 530,\n",
              " 're': 531,\n",
              " '10': 532,\n",
              " 'character': 533,\n",
              " 'prince': 534,\n",
              " 'voice': 535,\n",
              " 'god': 536,\n",
              " 'itself': 537,\n",
              " 'born': 538,\n",
              " 'fee': 539,\n",
              " 'didst': 540,\n",
              " 'five': 541,\n",
              " 'malone': 542,\n",
              " 'also': 543,\n",
              " 'o’': 544,\n",
              " 'friar': 545,\n",
              " 'silvia': 546,\n",
              " 'bertram': 547,\n",
              " '“and': 548,\n",
              " 'adriana': 549,\n",
              " 'crosses': 550,\n",
              " 'meet': 551,\n",
              " '7': 552,\n",
              " 'wrong': 553,\n",
              " 'sorrow': 554,\n",
              " '40': 555,\n",
              " 'marriage': 556,\n",
              " '65': 557,\n",
              " 'certain': 558,\n",
              " 'law': 559,\n",
              " 'slave': 560,\n",
              " 'henry': 561,\n",
              " 'including': 562,\n",
              " 'dinner': 563,\n",
              " 'date': 564,\n",
              " 'stage': 565,\n",
              " 'less': 566,\n",
              " 'says': 567,\n",
              " 'ears': 568,\n",
              " '25': 569,\n",
              " \"father's\": 570,\n",
              " 'person': 571,\n",
              " 'beauty': 572,\n",
              " '90': 573,\n",
              " 'known': 574,\n",
              " 'behind': 575,\n",
              " 'fall': 576,\n",
              " 'unless': 577,\n",
              " 'information': 578,\n",
              " 'felt': 579,\n",
              " 'julia': 580,\n",
              " 'gentle': 581,\n",
              " 'yourself': 582,\n",
              " 'fell': 583,\n",
              " '75': 584,\n",
              " 'making': 585,\n",
              " 'villain': 586,\n",
              " 'general': 587,\n",
              " 'permission': 588,\n",
              " 'court': 589,\n",
              " 'music': 590,\n",
              " \"cæsar's\": 591,\n",
              " 'www': 592,\n",
              " 'england': 593,\n",
              " 'stand': 594,\n",
              " 'honest': 595,\n",
              " 'tongue': 596,\n",
              " '55': 597,\n",
              " 'looked': 598,\n",
              " 'doubt': 599,\n",
              " 'wind': 600,\n",
              " 'phrase': 601,\n",
              " 'order': 602,\n",
              " 'power': 603,\n",
              " 'care': 604,\n",
              " 'associated': 605,\n",
              " 'domain': 606,\n",
              " 'pglaf': 607,\n",
              " '“if': 608,\n",
              " 'english': 609,\n",
              " 'present': 610,\n",
              " 'murder': 611,\n",
              " 'thoughts': 612,\n",
              " 'sun': 613,\n",
              " 'talk': 614,\n",
              " 'mercy': 615,\n",
              " 'lived': 616,\n",
              " 'lines': 617,\n",
              " 'john': 618,\n",
              " 'merry': 619,\n",
              " 'saying': 620,\n",
              " 'water': 621,\n",
              " 'warburton': 622,\n",
              " 'copies': 623,\n",
              " 'island': 624,\n",
              " 'juliet': 625,\n",
              " 'rosalind': 626,\n",
              " 'ang': 627,\n",
              " 'however': 628,\n",
              " '9': 629,\n",
              " 'spoke': 630,\n",
              " 'ere': 631,\n",
              " 'whether': 632,\n",
              " 'joy': 633,\n",
              " 'fellow': 634,\n",
              " 'effect': 635,\n",
              " '70': 636,\n",
              " 'madness': 637,\n",
              " 'katharine': 638,\n",
              " 'wood': 639,\n",
              " 'following': 640,\n",
              " 'killed': 641,\n",
              " 'p': 642,\n",
              " 'next': 643,\n",
              " 'ferdinand': 644,\n",
              " 'mr': 645,\n",
              " 'mar': 646,\n",
              " 'spirits': 647,\n",
              " 'freely': 648,\n",
              " 'common': 649,\n",
              " 'remember': 650,\n",
              " 'tears': 651,\n",
              " 'foul': 652,\n",
              " 'rich': 653,\n",
              " 'false': 654,\n",
              " 'moment': 655,\n",
              " 'children': 656,\n",
              " 'got': 657,\n",
              " 'often': 658,\n",
              " 'except': 659,\n",
              " 'brave': 660,\n",
              " 'four': 661,\n",
              " 'provide': 662,\n",
              " 'naples': 663,\n",
              " 'he’s': 664,\n",
              " '“the': 665,\n",
              " 'beautiful': 666,\n",
              " 'viola': 667,\n",
              " 'william': 668,\n",
              " 'anyone': 669,\n",
              " 'denmark': 670,\n",
              " 'ground': 671,\n",
              " '6': 672,\n",
              " 'break': 673,\n",
              " 'ear': 674,\n",
              " 'third': 675,\n",
              " 'near': 676,\n",
              " '80': 677,\n",
              " 'further': 678,\n",
              " 'receive': 679,\n",
              " 'save': 680,\n",
              " 'either': 681,\n",
              " 'feel': 682,\n",
              " 'ask': 683,\n",
              " 'fetch': 684,\n",
              " 'named': 685,\n",
              " 'distributing': 686,\n",
              " 'ebooks': 687,\n",
              " 'section': 688,\n",
              " 'http': 689,\n",
              " 'that’s': 690,\n",
              " 'u': 691,\n",
              " 'command': 692,\n",
              " 'wonder': 693,\n",
              " 'year': 694,\n",
              " 'farewell': 695,\n",
              " 'seems': 696,\n",
              " 'lose': 697,\n",
              " 'mark': 698,\n",
              " '105': 699,\n",
              " 'crown': 700,\n",
              " 'hither': 701,\n",
              " 'devil': 702,\n",
              " 'venice': 703,\n",
              " 'country': 704,\n",
              " 'patience': 705,\n",
              " 'answered': 706,\n",
              " 'beatrice': 707,\n",
              " 'portia': 708,\n",
              " \"o'er\": 709,\n",
              " 'guildenstern': 710,\n",
              " 'above': 711,\n",
              " 'sound': 712,\n",
              " 'suit': 713,\n",
              " 'goes': 714,\n",
              " 'neither': 715,\n",
              " 'shalt': 716,\n",
              " 'child': 717,\n",
              " 'gives': 718,\n",
              " 'ready': 719,\n",
              " 'ros': 720,\n",
              " 'lay': 721,\n",
              " 'fortune': 722,\n",
              " 'run': 723,\n",
              " 'measure': 724,\n",
              " 'wish': 725,\n",
              " 'happy': 726,\n",
              " 'written': 727,\n",
              " 'worse': 728,\n",
              " 'cry': 729,\n",
              " 'officer': 730,\n",
              " 'pericles': 731,\n",
              " 'petruchio': 732,\n",
              " 'distributed': 733,\n",
              " 'appeared': 734,\n",
              " 'french': 735,\n",
              " 'few': 736,\n",
              " 'fire': 737,\n",
              " 'seem': 738,\n",
              " 'virtue': 739,\n",
              " 'lies': 740,\n",
              " 'girl': 741,\n",
              " 'pay': 742,\n",
              " '85': 743,\n",
              " 'wilt': 744,\n",
              " 'revenge': 745,\n",
              " 'tale': 746,\n",
              " \"there's\": 747,\n",
              " 'age': 748,\n",
              " 'twenty': 749,\n",
              " 'vi': 750,\n",
              " 'send': 751,\n",
              " 'tax': 752,\n",
              " 'distribution': 753,\n",
              " 'fault': 754,\n",
              " 'luciana': 755,\n",
              " '“what': 756,\n",
              " 'replied': 757,\n",
              " 'pedro': 758,\n",
              " 'online': 759,\n",
              " 'rosencrantz': 760,\n",
              " 'subject': 761,\n",
              " 'high': 762,\n",
              " 'peace': 763,\n",
              " 'thyself': 764,\n",
              " 'flesh': 765,\n",
              " 'late': 766,\n",
              " '115': 767,\n",
              " 'boy': 768,\n",
              " 'passion': 769,\n",
              " 'dr': 770,\n",
              " 'among': 771,\n",
              " 'going': 772,\n",
              " 'dream': 773,\n",
              " 'service': 774,\n",
              " 'thank': 775,\n",
              " 'anything': 776,\n",
              " 'tempest': 777,\n",
              " 'door': 778,\n",
              " 'wine': 779,\n",
              " 'distribute': 780,\n",
              " 'comply': 781,\n",
              " 'web': 782,\n",
              " 'volunteers': 783,\n",
              " 'plutarch': 784,\n",
              " 'title': 785,\n",
              " '11': 786,\n",
              " 'majesty': 787,\n",
              " 'half': 788,\n",
              " 'cold': 789,\n",
              " 'watch': 790,\n",
              " 'duty': 791,\n",
              " '46': 792,\n",
              " 'arms': 793,\n",
              " 'sit': 794,\n",
              " 'holy': 795,\n",
              " 'shape': 796,\n",
              " '100': 797,\n",
              " '110': 798,\n",
              " 'times': 799,\n",
              " \"king's\": 800,\n",
              " 'question': 801,\n",
              " 'coming': 802,\n",
              " 'below': 803,\n",
              " 'strong': 804,\n",
              " 'town': 805,\n",
              " 'bade': 806,\n",
              " 'prose': 807,\n",
              " 'm': 808,\n",
              " 'stephano': 809,\n",
              " 'miranda': 810,\n",
              " 'dyce': 811,\n",
              " '’em': 812,\n",
              " 'least': 813,\n",
              " 'folio': 814,\n",
              " 'athens': 815,\n",
              " 'orlando': 816,\n",
              " '“a': 817,\n",
              " \"hamlet's\": 818,\n",
              " 'sight': 819,\n",
              " 'news': 820,\n",
              " 'pardon': 821,\n",
              " 'followed': 822,\n",
              " 'quarrel': 823,\n",
              " 'understand': 824,\n",
              " '95': 825,\n",
              " 'lead': 826,\n",
              " 'prison': 827,\n",
              " 'sorry': 828,\n",
              " 'swear': 829,\n",
              " 'ha': 830,\n",
              " 'open': 831,\n",
              " 'entered': 832,\n",
              " 'case': 833,\n",
              " 'b': 834,\n",
              " 'owner': 835,\n",
              " 'caliban': 836,\n",
              " 'lives': 837,\n",
              " 'there’s': 838,\n",
              " 'promised': 839,\n",
              " 'appears': 840,\n",
              " 'secret': 841,\n",
              " 'rome': 842,\n",
              " 'green': 843,\n",
              " 'taken': 844,\n",
              " 'france': 845,\n",
              " '33': 846,\n",
              " 'seek': 847,\n",
              " 'shows': 848,\n",
              " '38': 849,\n",
              " 'thunder': 850,\n",
              " 'change': 851,\n",
              " 'gentlemen': 852,\n",
              " 'youth': 853,\n",
              " 'plain': 854,\n",
              " 'draw': 855,\n",
              " 'pity': 856,\n",
              " 'probably': 857,\n",
              " 'wear': 858,\n",
              " 'poison': 859,\n",
              " 'round': 860,\n",
              " 'presently': 861,\n",
              " 'fish': 862,\n",
              " 'lest': 863,\n",
              " 'office': 864,\n",
              " 'touch': 865,\n",
              " 'fine': 866,\n",
              " 'trinculo': 867,\n",
              " 'comfort': 868,\n",
              " '235': 869,\n",
              " 'sake': 870,\n",
              " 'folios': 871,\n",
              " '“but': 872,\n",
              " 'n': 873,\n",
              " 'start': 874,\n",
              " 'produced': 875,\n",
              " '37': 876,\n",
              " 'side': 877,\n",
              " 'human': 878,\n",
              " 'history': 879,\n",
              " 'small': 880,\n",
              " 'volume': 881,\n",
              " '17': 882,\n",
              " 'soft': 883,\n",
              " 'unto': 884,\n",
              " 'ladies': 885,\n",
              " 'obey': 886,\n",
              " 'visit': 887,\n",
              " 'uncle': 888,\n",
              " 'wedding': 889,\n",
              " 'desire': 890,\n",
              " 'maid': 891,\n",
              " 'moon': 892,\n",
              " 'falling': 893,\n",
              " 'fit': 894,\n",
              " 'fly': 895,\n",
              " 'breath': 896,\n",
              " 'looks': 897,\n",
              " 'lover': 898,\n",
              " 'wished': 899,\n",
              " 'yours': 900,\n",
              " 'quite': 901,\n",
              " 'kill': 902,\n",
              " 'kings': 903,\n",
              " '135': 904,\n",
              " 'really': 905,\n",
              " 'support': 906,\n",
              " 'land': 907,\n",
              " 'wager': 908,\n",
              " 'fight': 909,\n",
              " 'osr': 910,\n",
              " 'compliance': 911,\n",
              " 'posted': 912,\n",
              " 'medium': 913,\n",
              " 'replacement': 914,\n",
              " 'limited': 915,\n",
              " 'errors': 916,\n",
              " 'gonzalo': 917,\n",
              " 'cell': 918,\n",
              " 'i’': 919,\n",
              " 'dryden': 920,\n",
              " 'company': 921,\n",
              " 'became': 922,\n",
              " 'honor': 923,\n",
              " 'olivia': 924,\n",
              " 'leonatus': 925,\n",
              " 'leonato': 926,\n",
              " 'appear': 927,\n",
              " 'deep': 928,\n",
              " 'original': 929,\n",
              " 'past': 930,\n",
              " 'ber': 931,\n",
              " 'quiet': 932,\n",
              " 'bid': 933,\n",
              " 'ho': 934,\n",
              " 'pale': 935,\n",
              " 'stood': 936,\n",
              " 'roman': 937,\n",
              " 'walk': 938,\n",
              " '26': 939,\n",
              " 'hard': 940,\n",
              " '29': 941,\n",
              " '44': 942,\n",
              " 'excellent': 943,\n",
              " 'glad': 944,\n",
              " '54': 945,\n",
              " 'report': 946,\n",
              " 'kept': 947,\n",
              " 'morning': 948,\n",
              " 'silence': 949,\n",
              " 'honour': 950,\n",
              " '74': 951,\n",
              " 'buy': 952,\n",
              " 'wit': 953,\n",
              " 'always': 954,\n",
              " 'wild': 955,\n",
              " '120': 956,\n",
              " 'need': 957,\n",
              " 'expression': 958,\n",
              " 'become': 959,\n",
              " 'dark': 960,\n",
              " 'nearly': 961,\n",
              " 'hate': 962,\n",
              " 'guil': 963,\n",
              " 'foolish': 964,\n",
              " 'hot': 965,\n",
              " 'try': 966,\n",
              " 'reading': 967,\n",
              " 'laugh': 968,\n",
              " 'ducats': 969,\n",
              " 'hark': 970,\n",
              " 'prithee': 971,\n",
              " 'morrow': 972,\n",
              " 'viii': 973,\n",
              " 'themselves': 974,\n",
              " 'using': 975,\n",
              " 'invisible': 976,\n",
              " 'messenger': 977,\n",
              " 'women': 978,\n",
              " 'win': 979,\n",
              " 'individual': 980,\n",
              " 'edd': 981,\n",
              " 'th’': 982,\n",
              " 'asleep': 983,\n",
              " 'sort': 984,\n",
              " 'leontes': 985,\n",
              " '“that': 986,\n",
              " 'diana': 987,\n",
              " 'aegeon': 988,\n",
              " 'language': 989,\n",
              " 'book': 990,\n",
              " 'kingdom': 991,\n",
              " 'street': 992,\n",
              " 'supposed': 993,\n",
              " 'lear': 994,\n",
              " 'collection': 995,\n",
              " 'black': 996,\n",
              " 'bitter': 997,\n",
              " '12': 998,\n",
              " '14': 999,\n",
              " '18': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "zQ-gHmw7yss7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ-gHmw7yss7",
        "outputId": "0b39d6f0-ae21-44fe-a08c-0eb538581436"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156671"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "CvOv8c6Sy9MN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CvOv8c6Sy9MN",
        "outputId": "77f4e9fc-8479-4235-9a90-d91002becfee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of hamlet, by william shakespeare\\n\\nthis ebook is for the use of anyone a'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "qaL2Oc0Iy3Zg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaL2Oc0Iy3Zg",
        "outputId": "2dc6d4a4-f4bf-461e-e099-394d6392f2a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "935886"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "4ff5fc8d0273709c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff5fc8d0273709c",
        "outputId": "eac12124-d3d2-44bb-f162-83ddc5dd0fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 56, 54, 411, 4, 107, 28, 668, 172, 20]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from itertools import chain\n",
        "\n",
        "# Choose an appropriate sequence length\n",
        "SEQ_LENGTH = 10\n",
        "\n",
        "# Convert the text to a list of sequences of numbers\n",
        "#sequences = None\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Make all sequences exactly 1000 words long\n",
        "X = [sequences[0][i:i+SEQ_LENGTH] for i in range(len(sequences[0])-SEQ_LENGTH)]\n",
        "# X = [list(chain(*sequences[i:i+S_LENGTH])) for i in range(len(sequences)-SEQ_LENGTH)]\n",
        "# X = [chain.from_iterable(sublist) for sublist in X]\n",
        "print(X[0])\n",
        "# x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Generate the training sequences\n",
        "# y = [sublist[0] if sublist else ' '  for sublist in sequences[1:]]\n",
        "y=sequences[0][SEQ_LENGTH:]\n",
        "# y=list(chain(*y))\n",
        "# y.append('0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "iBclz9rdGgoB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBclz9rdGgoB",
        "outputId": "19909fca-5e69-4cb1-ba1b-057a61d84209"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156661"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6bdc0deb930df1",
      "metadata": {
        "collapsed": false,
        "id": "3b6bdc0deb930df1"
      },
      "source": [
        "Assuming your sequences are stored in `X` and the corresponding targets in `y`, the following line should print the first training sequence and its target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "bMyKR8Gg2rD5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMyKR8Gg2rD5",
        "outputId": "a273ee2c-00f2-4d0f-dcfc-855251abf708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence: [2, 56, 54, 411, 4, 107, 28, 668, 172, 20]\n",
            "Target: 411\n",
            "Translated back to words: ['the', 'project', 'gutenberg', 'ebook', 'of', 'hamlet', 'by', 'william', 'shakespeare', 'this'] -> ebook\n"
          ]
        }
      ],
      "source": [
        "if len(X) > 0 and len(y) > 0:\n",
        "    print(f'Sequence: {X[0]}\\nTarget: {y[0]}')\n",
        "    print(f'Translated back to words: {[tokenizer.index_word[i] for i in X[0]]} -> {tokenizer.index_word[y[0]]}')\n",
        "else:\n",
        "    print(\"Training sequences have not been generated. Possible issue: Complete the relevant section of the assignment to initialize it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5bb2c55da17aaa0",
      "metadata": {
        "collapsed": false,
        "id": "d5bb2c55da17aaa0"
      },
      "source": [
        "And the following code will transform y into a one-hot encoded matrix, and split everything into training and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "3a929b2e6c2cc921",
      "metadata": {
        "id": "3a929b2e6c2cc921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (100571, 10)\n",
            "y_train shape: (100571, 1400)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Ensure that tokenizer has been initialized\n",
        "if tokenizer is not None:\n",
        "    # Convert X and y to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # One last thing: let's drop any examples where the target is the OOV token - we don't want our model to predict that (boring!)\n",
        "    if OOV_TOKEN in tokenizer.word_index:\n",
        "        mask = y != tokenizer.word_index[OOV_TOKEN]\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # One-hot encode the target token\n",
        "    y = to_categorical(y, num_classes=VOCAB_SIZE)\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f'X_train shape: {X_train.shape}')\n",
        "    print(f'y_train shape: {y_train.shape}')\n",
        "else:\n",
        "    print(\"Tokenizer has not been initialized. Please initialize it and load the vocabulary before continuing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fcb685",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b6e4161897210434",
      "metadata": {
        "collapsed": false,
        "id": "b6e4161897210434"
      },
      "source": [
        "# 2. Model Development (Complete or Incomplete)\n",
        "\n",
        "With the dataset prepared, it's time to develop the RNN model. You'll need to define the architecture of the model, compile it, and prepare it for training.\n",
        "\n",
        "## 2.1 Model Architecture\n",
        "\n",
        "Define the architecture of your RNN model. You can design it however you like, but there are a few features that it's important to include:\n",
        "\n",
        "*   An embedding layer that learns a dense representation of the input tokens. You'll need to specify the input dimension (the size of the vocabulary) and the output dimension (the size of the dense representation). Remember, you can look at the documentation [here](https://keras.io/api/layers/core_layers/embedding/).\n",
        "*   At least one recurrent layer. We have learned how to use LSTM layers in class, but you can use other types of recurrent layers if you prefer. You can find the documentation [here](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
        "*   A dense layer with a softmax activation function. This layer will output a probability distribution over the vocabulary, so that the model can make predictions about the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "06b0349c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D  \n",
        "VOCAB_SIZE=1400\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim= VOCAB_SIZE, output_dim=50, input_length=SEQ_LENGTH))\n",
        "GlobalAveragePooling1D(), \n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "\n",
        "# model = Sequential([\n",
        "#     Embedding(input_dim= VOCAB_SIZE, output_dim=100, input_length=MAX_SEQUENCE_LENGTH),\n",
        "#     LSTM(128, return_sequences=False),\n",
        "#     Dense(VOCAB_SIZE, activation='softmax')  # Specify number of units and activation function\n",
        "# ])\n",
        "\n",
        "# Check if the model has layers before trying to print the summary\n",
        "if len(model.layers) > 0:\n",
        "    model.summary()\n",
        "else:\n",
        "    print(\"No layers have been added to the model. Please complete the assignment by adding the required layers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fafd2dbb0d589fc",
      "metadata": {
        "collapsed": false,
        "id": "2fafd2dbb0d589fc"
      },
      "source": [
        "## 2.2 Model Compilation\n",
        "\n",
        "Compile the model with an appropriate loss function and optimizer. You might also want to track additional metrics, such as accuracy.\n",
        "\n",
        "Give a short explanation of your choice of loss function and optimizer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ae4ca7a12051b1fd",
      "metadata": {
        "id": "ae4ca7a12051b1fd"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    optimizer=Adam(learning_rate=0.01),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2f0b90a448c4f4b",
      "metadata": {
        "collapsed": false,
        "id": "c2f0b90a448c4f4b"
      },
      "source": [
        "## 2.3 Model Training\n",
        "\n",
        "Train the model on the training data you've prepared.\n",
        "\n",
        "* Train your model for 5 epochs with a batch size of 128. Use the validation data for validation.\n",
        "* Store the training history in a variable called `history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "256b1ea138c67ef7",
      "metadata": {
        "id": "256b1ea138c67ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m786/786\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 89ms/step - accuracy: 0.0739 - loss: 5.8016 - val_accuracy: 0.1266 - val_loss: 5.0614\n",
            "Epoch 2/5\n",
            "\u001b[1m786/786\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 80ms/step - accuracy: 0.1431 - loss: 4.8603 - val_accuracy: 0.1502 - val_loss: 4.8058\n",
            "Epoch 3/5\n",
            "\u001b[1m786/786\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 83ms/step - accuracy: 0.1713 - loss: 4.5002 - val_accuracy: 0.1639 - val_loss: 4.7061\n",
            "Epoch 4/5\n",
            "\u001b[1m786/786\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 83ms/step - accuracy: 0.1935 - loss: 4.2707 - val_accuracy: 0.1749 - val_loss: 4.6920\n",
            "Epoch 5/5\n",
            "\u001b[1m786/786\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 86ms/step - accuracy: 0.2089 - loss: 4.1072 - val_accuracy: 0.1793 - val_loss: 4.7027\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "history = model.fit(\n",
        "    X_train, y_train,validation_split=0.1,  # Training data\n",
        "    epochs=5,          # Number of epochs\n",
        "    batch_size=128,    # Batch size\n",
        "    validation_data=(X_val, y_val)  # Validation data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195c59bf80d2a2c4",
      "metadata": {
        "collapsed": false,
        "id": "195c59bf80d2a2c4"
      },
      "source": [
        "Plot the training history to visualize the model's learning progress. Your plot should include the training and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "9e8cacec70d8f313",
      "metadata": {
        "id": "9e8cacec70d8f313"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your code here\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your code here\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9ed71305787aed",
      "metadata": {
        "collapsed": false,
        "id": "3d9ed71305787aed"
      },
      "source": [
        "# 3. Text Generation (Complete or Incomplete)\n",
        "\n",
        "Write a method called `generate_text` that uses the trained model to generate new text. The method should take the following parameters:\n",
        "\n",
        "*   `model`: The trained RNN model.\n",
        "*   `tokenizer`: The tokenizer used to pre-process the text data.\n",
        "*   `seed_text`: The seed text the model will use to generate new text.\n",
        "*   `max_sequence_len`: The maximum length of the sequence used to generate new text.\n",
        "\n",
        "The method should return the generated text.\n",
        "\n",
        "An overview of the text generation process you should follow:\n",
        "\n",
        "1. Tokenize the seed text using the tokenizer we built before.\n",
        "2. Pad the sequences to the same length as the training sequences - you can use the `pad_sequences` method from the `keras.preprocessing.sequence` module, which is documented [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).\n",
        "3. Use the model to predict the next token in the sequence. Remember that the model will output a probability distribution over the vocabulary, so you'll need to use `np.argmax` to find the token with the highest probability.\n",
        "4. Add the predicted token to the sequence and remove the first token.\n",
        "5. Repeat steps 3-4 until you have generated the desired number of tokens.\n",
        "6. Convert the generated token IDs back to words and return the combined result as a single string.\n",
        "\n",
        "This is a challenging task, so don't hesitate to ask for help if you need it. It's okay if the generated text doesn't make much sense yet - we'll work on improving the model next.\n",
        "As a bonus, you can make your method generate \"gpt-style\" by having it print out each word as it's generated, so you can see the text being generated in real time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "d73dbf278a1265ef",
      "metadata": {
        "id": "d73dbf278a1265ef"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def get_predicted_word(model, sequence):\n",
        "    \"\"\"\n",
        "    Get the predicted word from the model.\n",
        "    This helper function uses a concept we haven't covered in class yet: temperature.\n",
        "    In essence, a little bit of randomness in what word we predict can make the text more interesting.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use the model to predict the next token in the sequence\n",
        "    yhat = model.predict(sequence, verbose=0)\n",
        "\n",
        "    # Get the index of the predicted word, according to the probabilities\n",
        "    yhat = np.random.choice(range(VOCAB_SIZE), p=yhat.ravel())\n",
        "\n",
        "    return yhat\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Generate new text using the trained model.\n",
        "    You can use the `get_predicted_word` helper function to help you with this.\n",
        "    \"\"\"\n",
        "    # Tokenize the seed text\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    generated_text = seed_text\n",
        "    \n",
        "    for _ in range(max_sequence_len):\n",
        "        # Pad the sequences to the same length as the training sequences\n",
        "        token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "        # Use the model to predict the next token in the sequence\n",
        "        predicted_token = get_predicted_word(model, token_list_padded)\n",
        "\n",
        "        # Convert the predicted token index back to the word\n",
        "        predicted_word = tokenizer.index_word[predicted_token]\n",
        "        \n",
        "        # Append the predicted word to the sequence\n",
        "        token_list.append(predicted_token)\n",
        "        generated_text += ' ' + predicted_word\n",
        "        \n",
        "        # Remove the first token\n",
        "        token_list = token_list[1:]\n",
        "    return generate_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26d120c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "f463b0c3df49e2c",
      "metadata": {
        "id": "f463b0c3df49e2c"
      },
      "outputs": [],
      "source": [
        "# Test the text generation function\n",
        "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5871d836a0135c41",
      "metadata": {
        "collapsed": false,
        "id": "5871d836a0135c41"
      },
      "source": [
        "It's likely that the text generated by your model doesn't make much sense yet. This is because the model hasn't been trained for very long, and the training dataset is relatively small.\n",
        "\n",
        "# 4. Model Refinement (Complete or Incomplete)\n",
        "\n",
        "In this last section, you'll work on improving your model. There are many ways to do this, but here are a few ideas to get you started:\n",
        "\n",
        "* Use pre-trained embeddings: the code below will help you to load pre-trained embeddings through Keras.\n",
        "* Experiment with different model architectures, including the number of layers, the number of units in each layer, and the use of dropout layers.\n",
        "* Train your model for longer. You can also experiment with different batch sizes.\n",
        "\n",
        "Implement and test out at least one of these ideas. If you have other ideas for improving the model, feel free to try them out as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "dda8b0f845c20862",
      "metadata": {
        "id": "dda8b0f845c20862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: wget: command not found\n",
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "e8b777220505635",
      "metadata": {
        "id": "e8b777220505635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'glove.6B.100d.txt' File not found. Please make sure you have ran the previous cell.\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained embeddings\n",
        "embeddings_index = {}\n",
        "try:\n",
        "    with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    print(f'Found {len(embeddings_index)} word vectors.')\n",
        "except FileNotFoundError:\n",
        "    print(\"'glove.6B.100d.txt' File not found. Please make sure you have ran the previous cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "ac1ae0c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "def download_glove_embeddings(url, extract_to='.'):\n",
        "    response = requests.get(url)\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        z.extractall(extract_to)\n",
        "\n",
        "# URL for GloVe embeddings\n",
        "glove_url = \"https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\n",
        "download_glove_embeddings(glove_url, extract_to='glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "d3e48ff004757cf2",
      "metadata": {
        "id": "d3e48ff004757cf2"
      },
      "outputs": [],
      "source": [
        "# Check if VOCAB_SIZE is set\n",
        "if VOCAB_SIZE is None:\n",
        "    print(\"You need to complete the previous parts of your assignment in order for this to work.\")\n",
        "else:\n",
        "    # Create an embedding matrix\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < VOCAB_SIZE:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "e3d21d5dbbbcf9f9",
      "metadata": {
        "id": "e3d21d5dbbbcf9f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sitongye/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Check if VOCAB_SIZE is set\n",
        "if VOCAB_SIZE is None:\n",
        "    print(\"You need to complete the previous parts of your assignment in order for this to work.\")\n",
        "else:\n",
        "    embedding_layer = Embedding(\n",
        "        VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=SEQ_LENGTH, trainable=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "236cb723e4e5b3fc",
      "metadata": {
        "id": "236cb723e4e5b3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 314ms/step - accuracy: 0.0473 - loss: 6.2268 - val_accuracy: 0.0523 - val_loss: 5.9064\n",
            "Epoch 2/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 303ms/step - accuracy: 0.0524 - loss: 5.9341 - val_accuracy: 0.0523 - val_loss: 5.8986\n",
            "Epoch 3/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 303ms/step - accuracy: 0.0524 - loss: 5.9329 - val_accuracy: 0.0523 - val_loss: 5.9025\n",
            "Epoch 4/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 299ms/step - accuracy: 0.0530 - loss: 5.9260 - val_accuracy: 0.0523 - val_loss: 5.8980\n",
            "Epoch 5/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 301ms/step - accuracy: 0.0533 - loss: 5.9183 - val_accuracy: 0.0523 - val_loss: 5.8941\n",
            "Epoch 6/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 301ms/step - accuracy: 0.0525 - loss: 5.9289 - val_accuracy: 0.0523 - val_loss: 5.8977\n",
            "Epoch 7/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 303ms/step - accuracy: 0.0541 - loss: 5.9139 - val_accuracy: 0.0523 - val_loss: 5.8989\n",
            "Epoch 8/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 308ms/step - accuracy: 0.0530 - loss: 5.9217 - val_accuracy: 0.0523 - val_loss: 5.8939\n",
            "Epoch 9/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 883ms/step - accuracy: 0.0520 - loss: 5.9182 - val_accuracy: 0.0523 - val_loss: 5.8962\n",
            "Epoch 10/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 424ms/step - accuracy: 0.0526 - loss: 5.9308 - val_accuracy: 0.0523 - val_loss: 5.8951\n",
            "Epoch 11/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 427ms/step - accuracy: 0.0525 - loss: 5.9159 - val_accuracy: 0.0523 - val_loss: 5.8947\n",
            "Epoch 12/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 424ms/step - accuracy: 0.0540 - loss: 5.9141 - val_accuracy: 0.0523 - val_loss: 5.8940\n",
            "Epoch 13/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 430ms/step - accuracy: 0.0537 - loss: 5.9206 - val_accuracy: 0.0523 - val_loss: 5.8969\n",
            "Epoch 14/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 419ms/step - accuracy: 0.0537 - loss: 5.9148 - val_accuracy: 0.0523 - val_loss: 5.8961\n",
            "Epoch 15/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 419ms/step - accuracy: 0.0522 - loss: 5.9169 - val_accuracy: 0.0523 - val_loss: 5.8954\n",
            "Epoch 16/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 421ms/step - accuracy: 0.0526 - loss: 5.9224 - val_accuracy: 0.0523 - val_loss: 5.8945\n",
            "Epoch 17/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 420ms/step - accuracy: 0.0522 - loss: 5.9333 - val_accuracy: 0.0523 - val_loss: 5.8940\n",
            "Epoch 18/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 409ms/step - accuracy: 0.0512 - loss: 5.9230 - val_accuracy: 0.0523 - val_loss: 5.8929\n",
            "Epoch 19/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 646ms/step - accuracy: 0.0532 - loss: 5.9080 - val_accuracy: 0.0523 - val_loss: 5.8953\n",
            "Epoch 20/20\n",
            "\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 402ms/step - accuracy: 0.0530 - loss: 5.9149 - val_accuracy: 0.0523 - val_loss: 5.8938\n"
          ]
        }
      ],
      "source": [
        "# Define a new model and train it\n",
        "\n",
        "# Your code here\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "GlobalAveragePooling1D(), \n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split=0.1,\n",
        "          epochs=20, batch_size=256) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742095fb",
      "metadata": {
        "id": "742095fb"
      },
      "source": [
        "## Criteria\n",
        "\n",
        "|Criteria|Complete|Incomplete|\n",
        "|----|----|----|\n",
        "|Task 1|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
        "|Task 2|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
        "|Task 3|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|\n",
        "|Task 4|The task has been completed successfully and there are no errors.|The task is still incomplete and there is at least one error.|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1990e2d2",
      "metadata": {
        "id": "1990e2d2"
      },
      "source": [
        "## Submission Information\n",
        "\n",
        "🚨**Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)**🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
        "\n",
        "### Submission Parameters:\n",
        "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
        "* The branch name for your repo should be: `assignment-2`\n",
        "* What to submit for this assignment:\n",
        "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
        "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/deep_learning/pull/<pr_id>`\n",
        "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
        "\n",
        "Checklist:\n",
        "- [ ] Created a branch with the correct naming convention.\n",
        "- [ ] Ensured that the repository is public.\n",
        "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
        "- [ ] Verify that the link is accessible in a private browser window.\n",
        "\n",
        "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
